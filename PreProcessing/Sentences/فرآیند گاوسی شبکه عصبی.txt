کل یا بخشی از این مقاله به زبانی به‌جز زبان فارسی نوشته شده‌است. اگر مقصود ارائهٔ مقاله برای مخاطبان آن زبان است، باید در نسخه‌ای از ویکی‌پدیا به همان زبان نوشته شود (فهرست ویکی‌پدیاها را ببینید). در غیر این صورت، خواهشمند است ترجمهٔ این مقاله را با توجه به متن اصلی و با رعایت سیاست ویرایش، دستور خط فارسی و برابر سازی به زبان فارسی بهبود دهید و سپس این الگو را از بالای صفحه بردارید. همچنین برای بحث‌های مرتبط، مدخل این مقاله در فهرست صفحه‌های نیازمند ترجمه به فارسی را ببینید. اگر این مقاله به زبان فارسی بازنویسی نشود، تا دو هفتهٔ دیگر نامزد حذف می‌شود و/یا به نسخهٔ زبانی مرتبط ویکی‌پدیا منتقل خواهد شد.
اگر شما اخیراً این مقاله را به‌عنوان صفحهٔ نیازمند ترجمه برچسب زده‌اید، لطفاً عبارت{{جا:نیاز به ترجمه &#124;صفحه=فرآیند گاوسی شبکه عصبی&#160;&#124;زبان=نامشخص&#160;&#124;نظر=&#160;}} ~~~~را به پایین این بخش از فهرست صفحه‌های نیازمند ترجمه به فارسی بیفزایید.&#160;لطفاً 
شبکه‌های بیزی ابزار مدل‌سازی برای تخصیص احتمالات به رویدادها و در نتیجه مشخص کردن احتمال درستی یا نادرستی در پیش‌بینی‌های یک مدل هستند. یادگیری عمیق و شبکه‌های عصبی رویکردهایی هستند که در یادگیری ماشین برای ساخت مدل‌های محاسباتی استفاده می‌شوند که از داده‌های آموزشی یادمی‌گیرند و با داده‌های تست ارزیابی می‌شوند. شبکه‌های عصبی بیزی این دو رشته را ادغام می‌کنند. آنها نوعی شبکه عصبی هستند که پارامترها و پیش‌بینی‌های آنها هر دو احتمالاتی هستند. در حالی که شبکه‌های عصبی معمولی اغلب پیش‌بینی‌های قطعی بعنی صفر و یک دارند، شبکه‌های عصبی بیزی می‌توانند با دقت بیشتری و به صورت غیر قطعی احتمال درست بودن پیش‌بینی‌هایشان را ارزیابی کنند.
فرآیندهای گاوسی شبکه عصبی (NNGPs) تا حدی معادل شبکه‌های عصبی بیزی هستند، و یک شکل بسته ارائه می‌کنند. روشی برای ارزیابی شبکه‌های عصبی بیزی آنها یک توزیع احتمال فرایند گاوسی هستند که توزیع بر روی پیش‌بینی‌های انجام شده توسط شبکه عصبی بیزی مربوطه را توصیف می‌کند. محاسبات در شبکه‌های عصبی مصنوعی معمولاً در لایه‌های متوالی نورون‌های مصنوعی سازماندهی می‌شوند. به تعداد نورون‌های یک لایه، عرض لایه می‌گویند. هم‌ارزی بین NNGPها و شبکه‌های عصبی بیزی زمانی اتفاق می‌افتد که لایه‌ها در شبکه عصبی بیزی بی‌نهایت گسترده شوند (شکل را ببینید). این محدودیت عرض بزرگ مورد توجه عملی است، زیرا شبکه‌های عصبی با عرض محدود معمولاً با افزایش عرض لایه عملکرد بهتری دارند.
NNGP همچنین در زمینه‌های دیگر ظاهر می‌شود: توزیع را بر روی پیش‌بینی‌های انجام شده توسط شبکه‌های غیر بیزی پس از مقداردهی اولیه رندوم پارامترهای این شبکه‌ها، توصیف می‌کند، اما این اتفاق قبل از یادگیری شبکه رخ می‌دهد. به عنوان یک اصطلاح در هسته مماس عصبی معادلات پیش‌بینی ظاهر می‌شود. از شبکه گاوسی در انتشار اطلاعات عمیق استفاده می‌شود تا مشخص شود که آیا هایپرپارامترها و معماری‌ها قابلیت بادگیری دارند یا خیر. این به محدودیت‌های تعداد نورون‌های شبکه‌های عصبی مربوط می‌شود.
هر تنظیم پارامترهای یک شبکه عصبی                         &#x03B8;                 {\displaystyle \theta }     مربوط به یک تابع خاص است که توسط شبکه عصبی محاسبه می‌شود. توزیع ابتدایی                         p         (         &#x03B8;         )                 {\displaystyle p(\theta )}      روی پارامترهای شبکه عصبی مربوط به یک توزیع بر روی توابع محاسبه شده توسط شبکه است. از آنجایی که شبکه‌های عصبی بی‌نهایت گسترده هستند، این توزیع بر روی توابع برای بسیاری از معماری‌ها به یک فرایند گاوسی همگرا می‌شود.
شکل سمت راست خروجی‌های یک بعدی را ترسیم می‌کند                                    z                        L                             (         &#x22C5;         ;         &#x03B8;         )                 {\displaystyle z^{L}(\cdot ;\theta )}     یک شبکه عصبی برای دو ورودی                         x                 {\displaystyle x}     و                                    x                        &#x2217;                                     {\displaystyle x^{*}}     در برابر یکدیگر نقاط سیاه تابع محاسبه شده توسط شبکه عصبی روی این ورودی‌ها را برای ترسیم تصادفی پارامترها از                         p         (         &#x03B8;         )                 {\displaystyle p(\theta )}     . خطوط قرمز، خطوط همسان احتمال برای توزیع مشترک بر روی خروجی‌های شبکه یعنی                                    z                        L                             (         x         ;         &#x03B8;         )                 {\displaystyle z^{L}(x;\theta )}     و                                    z                        L                             (                    x                        &#x2217;                             ;         &#x03B8;         )                 {\displaystyle z^{L}(x^{*};\theta )}     هستند که از توزیع                          p         (         &#x03B8;         )                 {\displaystyle p(\theta )}     القا شده‌است. . این توزیع در فضای تابع مربوط به توزیع                         p         (         &#x03B8;         )                 {\displaystyle p(\theta )}      در فضای پارامتر است، و نقاط سیاه نمونه‌هایی از این توزیع هستند. برای شبکه‌های عصبی بی‌نهایت گسترده، از آنجایی که توزیع روی توابع محاسبه شده توسط شبکه عصبی یک فرایند گاوسی است، توزیع مشترک بر روی خروجی‌های یک شبکه گاوسی چند متغیره برای هر مجموعه متناهی از ورودی‌های شبکه است.
نماد استفاده شده در این بخش مانند نماد استفاده شده در زیر برای به دست آوردن مطابقت بین NNGPها و شبکه‌های کاملاً همبند است و جزئیات بیشتر نیز قابل مشاهده است..
نشان داده شده‌است که هم‌ارزی بین شبکه‌های عصبی بیزینی با پهنای بی‌نهایت و NNGP برای: شبکه‌های کاملاً متصل و عمیق برقرار است، زیرا تعداد واحدها در هر لایه به بی‌نهایت می‌رسد. شبکه‌های عصبی کانولوشن به عنوان تعداد کانال‌ها تا بی‌نهایت گرفته می‌شود. شبکه‌های ترانسفورماتور به عنوان تعداد سر توجه به بی‌نهایت گرفته می‌شود. شبکه‌های تکراری به عنوان تعداد واحدها تا بی‌نهایت گرفته می‌شود. در واقع، این تناظر NNGP تقریباً برای هر معماری صادق است: به‌طور کلی، اگر یک معماری را بتوان صرفاً از طریق ضرب ماتریس و غیرخطی‌های هماهنگی بیان کرد (یعنی یک برنامه تانسور)، آنگاه دارای یک GP با عرض نامحدود است. این به‌طور خاص شامل تمام شبکه‌های عصبی پیش‌خور یا بازگشتی متشکل از پرسپترون چندلایه، شبکه‌های عصبی بازگشتی (مثلاً LSTMs، درنا)، (دوم یا نمودار) پیچیدگی، تجمع، جست و خیز اتصال، توجه، عادی دسته ای، و / یا لایه عادی.
معماری‌های خاصی را می‌توان طوری تغییر داد که به شبکه‌های عصبی بیزی نامتناهی اجازه دهد که NNGP با هسته‌های ترکیب شده از طریق عملیات افزایشی و ضربی تولید کنند.
این بخش در مورد تطابق بین شبکه‌های عصبی گسترده و فرآیندهای گاوسی برای مورد خاص یک معماری کاملاً متصل گسترش می‌یابد. این یک طرح اثباتی ارائه می‌دهد که نشان می‌دهد چرا مکاتبات برقرار است، و شکل عملکرد خاص NNGP را برای شبکه‌های کاملاً متصل معرفی می‌کند. طرح اثبات نزدیک به رویکرد نواک و همکاران است..
یک شبکه عصبی مصنوعی کاملاً متصل با ورودی‌ها را در نظر بگیرید                         x                 {\displaystyle x}     ، مولفه‌های                         &#x03B8;                 {\displaystyle \theta }     متشکل از وزنه‌ها                                    W                        l                                     {\displaystyle W^{l}}     و تعصبات                                    b                        l                                     {\displaystyle b^{l}}     برای هر لایه                         l                 {\displaystyle l}     در شبکه، پیش فعال سازی (پیش غیرخطی)                                    z                        l                                     {\displaystyle z^{l}}     ، فعال سازی (پس از غیرخطی بودن)                                    y                        l                                     {\displaystyle y^{l}}     ، غیرخطی نقطه ای                         &#x03D5;         (         &#x22C5;         )                 {\displaystyle \phi (\cdot )}     و عرض لایه‌ها                                    n                        l                                     {\displaystyle n^{l}}     . برای سادگی، عرض                                    n                        L             +             1                                     {\displaystyle n^{L+1}}     از بردار بازخوانی                                    z                        L                                     {\displaystyle z^{L}}     1 در نظر گرفته شده‌است. پارامترهای این شبکه دارای توزیع قبلی هستند                         p         (         &#x03B8;         )                 {\displaystyle p(\theta )}     ، که از یک گاوسی همسانگرد برای هر وزن و بایاس تشکیل شده‌است، با واریانس وزن‌ها به صورت معکوس با عرض لایه. این شبکه در شکل سمت راست نشان داده شده‌است و با مجموعه معادلات زیر توضیح داده شده‌است:
ابتدا مشاهده می‌کنیم که پیش فعال سازی‌ها                                    z                        l                                     {\displaystyle z^{l}}     توسط یک فرایند گاوسی مشروط به فعال سازی‌های قبلی توصیف می‌شوند                                    y                        l                                     {\displaystyle y^{l}}     . این نتیجه حتی در عرض محدود نیز برقرار است. هر پیش فعال سازی                                    z                        i                                   l                                     {\displaystyle z_{i}^{l}}     یک مجموع وزنی از متغیرهای تصادفی گاوسی است که مربوط به اوزان است                                    W                        i             j                                   l                                     {\displaystyle W_{ij}^{l}}     و تعصبات                                    b                        i                                   l                                     {\displaystyle b_{i}^{l}}     ، که در آن ضرایب هر یک از آن متغیرهای گاوسی، فعال سازی‌های قبلی هستند                                    y                        j                                   l                                     {\displaystyle y_{j}^{l}}     . از آنجایی که آنها مجموع وزنی از گاوسیان با میانگین صفر هستند                                    z                        i                                   l                                     {\displaystyle z_{i}^{l}}     خودشان گاوسیان صفر میانگین هستند (مشروط به ضرایب                                    y                        j                                   l                                     {\displaystyle y_{j}^{l}}    ). از آنجا که                                    z                        l                                     {\displaystyle z^{l}}     به‌طور مشترک برای هر مجموعه ای از گاوسی هستند                                    y                        l                                     {\displaystyle y^{l}}     ، آنها توسط یک فرایند گاوسی مشروط به فعال سازی‌های قبلی توصیف می‌شوند                                    y                        l                                     {\displaystyle y^{l}}     . کوواریانس یا هسته این فرایند گاوسی به وزن و واریانس بایاس بستگی دارد                                    &#x03C3;                        w                                   2                                     {\displaystyle \sigma _{w}^{2}}     و                                    &#x03C3;                        b                                   2                                     {\displaystyle \sigma _{b}^{2}}     و همچنین ماتریس لحظه دوم                                    K                        l                                     {\displaystyle K^{l}}     از فعال سازی‌های قبلی                                    y                        l                                     {\displaystyle y^{l}}    .
تأثیر ترازو وزن                                    &#x03C3;                        w                                   2                                     {\displaystyle \sigma _{w}^{2}}     این است که سهم به ماتریس کوواریانس را مجدداً مقیاس بندی کنیم                                    K                        l                                     {\displaystyle K^{l}}     ، در حالی که سوگیری برای همه ورودی‌ها مشترک است و غیره                                    &#x03C3;                        b                                   2                                     {\displaystyle \sigma _{b}^{2}}     را می‌سازد                                    z                        i                                   l                                     {\displaystyle z_{i}^{l}}     برای نقاط داده مختلف شبیه تر است و ماتریس کوواریانس را بیشتر شبیه به یک ماتریس ثابت می‌کند.
پیش فعال سازی‌ها                                    z                        l                                     {\displaystyle z^{l}}     فقط به                                    y                        l                                     {\displaystyle y^{l}}     از طریق ماتریس لحظه دوم آن                                    K                        l                                     {\displaystyle K^{l}}     وابسته است. به همین دلیل می‌توان گفت که                                    z                        l                                     {\displaystyle z^{l}}     یک فرایند گاوسی شرطی روی                                    K                        l                                     {\displaystyle K^{l}}     است، نه روی                                    y                        l                                     {\displaystyle y^{l}}    .
همان‌طور که قبلاً تعریف شد،                                    K                        l                                     {\displaystyle K^{l}}     ماتریس لحظه دوم از                                    y                        l                                     {\displaystyle y^{l}}     . از آنجا که                                    y                        l                                     {\displaystyle y^{l}}     بردار فعال سازی پس از اعمال غیرخطی است                         &#x03D5;                 {\displaystyle \phi }     ، می‌توان آن را جایگزین کرد                         &#x03D5;                    (                        z                            l               &#x2212;               1                                   )                          {\displaystyle \phi \left(z^{l-1}\right)}     ، و در نتیجه یک معادله اصلاح شده بیان می‌کند                                    K                        l                                     {\displaystyle K^{l}}     برای                         l         &gt;         0                 {\displaystyle l&gt;0}     به لحاظ                                    z                        l             &#x2212;             1                                     {\displaystyle z^{l-1}}     ،
ما قبلاً تعیین کرده‌ایم که                                    z                        l             &#x2212;             1                                        |                             K                        l             &#x2212;             1                                     {\displaystyle z^{l-1}|K^{l-1}}     یک فرایند گاوسی است. این یعنی مجموعی که                                     K                        l                                     {\displaystyle K^{l}}     را تعریف می‌کند، میانگین                                    n                        l                                     {\displaystyle n^{l}}     نمونه ای از یک فرایند گاوسی است که خود تابعی از                                    K                        l             &#x2212;             1                                     {\displaystyle K^{l-1}}     است.
                                                                                                   {                                                               z                                                i                                                                       l                         &#x2212;                         1                                                                 (                     x                     )                     ,                                            z                                                i                                                                       l                         &#x2212;                         1                                                                 (                                            x                       &#x2032;                                          )                                      }                                                                                 &#x223C;                                                         G                     P                                                                        (                                        0                     ,                                            &#x03C3;                                                w                                                                       2                                                                                        K                                                l                         &#x2212;                         1                                                                 +                                            &#x03C3;                                                b                                                                       2                                                                                  )                                  .                                                                 {\displaystyle {\begin{aligned}\left\{z_{i}^{l-1}(x),z_{i}^{l-1}(x')\right\}&amp;\sim {\mathcal {GP}}\left(0,\sigma _{w}^{2}K^{l-1}+\sigma _{b}^{2}\right).\end{aligned}}}    
به عنوان عرض لایه                                    n                        l                                     {\displaystyle n^{l}}     به بی‌نهایت می‌رود، این میانگین به پایان می‌رسد                                    n                        l                                     {\displaystyle n^{l}}     نمونه‌هایی از فرایند گاوسی را می‌توان با یک انتگرال بر روی فرایند گاوسی جایگزین کرد:
بنابراین، در محدوده عرض نامحدود ماتریس لحظه دوم                                    K                        l                                     {\displaystyle K^{l}}     برای هر جفت ورودی                         x                 {\displaystyle x}     و                                    x           &#x2032;                          {\displaystyle x'}     را می‌توان به عنوان یک انتگرال بر روی یک گاوسی ۲ بعدی، از حاصل ضرب بیان کرد                         &#x03D5;         (         z         )                 {\displaystyle \phi (z)}     و                         &#x03D5;         (                    z           &#x2032;                  )                 {\displaystyle \phi (z')}     . تعدادی از موقعیت‌ها وجود دارد که در آن این به صورت تحلیلی حل شده‌است، مانند زمانی که                         &#x03D5;         (         &#x22C5;         )                 {\displaystyle \phi (\cdot )}     یک تابع غیرخطی ReLU , ELU, GELU، یا خطا. حتی زمانی که نمی‌توان آن را به صورت تحلیلی حل کرد، از آنجایی که یک انتگرال ۲ بعدی است، به‌طور کلی می‌توان آن را به صورت عددی به‌طور مؤثر محاسبه کرد. این انتگرال قطعی است، بنابراین                                    K                        l                                        |                             K                        l             &#x2212;             1                                     {\displaystyle K^{l}|K^{l-1}}     قطعی است. برای کوتاه نویسی یک تابع تعریف می‌کنیم                         F                 {\displaystyle F}     ، که مربوط به محاسبه این انتگرال ۲ بعدی برای همه جفت ورودی‌ها و نقشه‌ها است                                    K                        l             &#x2212;             1                                     {\displaystyle K^{l-1}}     به                                    K                        l                                     {\displaystyle K^{l}}     ،
با اعمال این مشاهده به صورت بازگشتی که                                    K                        l                             &#x2223;                    K                        l             &#x2212;             1                                     {\displaystyle K^{l}\mid K^{l-1}}     وقتی                                     n                        l                             &#x2192;         &#x221E;                 {\displaystyle n^{l}\rightarrow \infty }    ، قطعی است،                                    K                        L                                     {\displaystyle K^{L}}     را می‌توان به عنوان یک تابع قطعی                                    K                        0                                     {\displaystyle K^{0}}     در نظر گرفت،
جایی که                                    F                        L                                     {\displaystyle F^{L}}     نشان دهنده اعمال تابعی است                         F                 {\displaystyle F}     به صورت متوالی                         L                 {\displaystyle L}     بار. با ترکیب این عبارت با مشاهدات بعدی که لایه ورودی ماتریس گشتاور دوم است                                    K                        0                             (         x         ,                    x           &#x2032;                  )         =                                 1                            n                                0                                                                    &#x2211;                        i                                        x                        i                                        x                        i                      &#x2032;                          {\displaystyle K^{0}(x,x')={\frac {1}{n^{0}}}\sum _{i}x_{i}x'_{i}}     تابع قطعی ورودی است                         x                 {\displaystyle x}     ، و آن                                    z                        L                                        |                             K                        L                                     {\displaystyle z^{L}|K^{L}}     یک فرایند گاوسی است، خروجی شبکه عصبی را می‌توان به عنوان یک فرایند گاوسی بر حسب ورودی آن بیان کرد،
Neural Tangents یک کتابخانه رایگان و منبع باز پایتون است که برای محاسبه و انجام استنتاج با NNGP و هسته مماس عصبی مربوط به معماری‌های مختلف ANN رایج استفاده می‌شود.
