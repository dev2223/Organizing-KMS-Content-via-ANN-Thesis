ترسنفورمر تولیدگر از پیش آموزش دیده به اختصار (GPT-3) یک مدل زبانی خودگرا است که از یادگیری عمیق برای تولید متنی متنی شبیه انسان استفاده می‌کند.
این سومین مدل پیش‌بینی زبان در سری GPT-n (و جانشین GPT-2) است که توسط اوپن ای‌آی، آزمایشگاه تحقیقاتی هوش مصنوعی مستقر در سانفرانسیسکو ایجاد شده‌است. نسخه کامل GPT-3 دارای ظرفیت ۱۷۵ میلیارد پارامتر یادگیری ماشین است. GPT-3، که در ماه مه ۲۰۲۰ معرفی شد و از ژوئیه ۲۰۲۰ در مرحله آزمایشی قرار دارد، بخشی از روند سیستم‌های پردازش زبان طبیعی (NLP) در ارائه زبان از پیش آموزش دیده‌است.
کیفیت متن تولید شده توسط GPT-3 به حدی بالا است که تعیین اینکه آیا توسط انسان نوشته شده‌است یا خیر، دشوار است، که دارای مزایا و خطرات است. سی و یک محقق و مهندس اوپن ای‌آی مقاله اصلی خود را در ۲۸ می ۲۰۲۰ معرفی کرد که GPT-3 را معرفی می‌کرد. آنها در مقاله خود در مورد خطرات احتمالی GPT-3 هشدار دادند و خواستار تحقیق برای کاهش خطر شدند. &#58;&#8202;34&#8202;دیوید چالمرز، فیلسوف استرالیایی، GPT-3 را «یکی از جالب‌ترین و مهمترین سیستم‌های هوش مصنوعی تولید شده تا به حال» توصیف کرد.
مایکروسافت در ۲۲ سپتامبر ۲۰۲۰ اعلام کرد که مجوز استفاده «انحصاری» از GPT-3 را دریافت کرده‌است. دیگران هنوز می‌توانند از واسط برنامه‌نویسی کاربردی عمومی برای دریافت خروجی استفاده کنند، اما فقط مایکروسافت به مدل اصلی GPT-3 دسترسی دارد.
به گفته اکونومیست، بهبود الگوریتم‌ها، رایانه‌های قدرتمند و افزایش داده‌های دیجیتالی باعث انقلاب در یادگیری ماشین شده‌است، با تکنیک‌های جدید در دهه ۲۰۱۰ منجر به «پیشرفت سریع در وظایف» از جمله دستکاری زبان شد. مدلهای نرم‌افزاری برای یادگیری با استفاده از هزاران یا میلیونها مثال در یک ساختار بر اساس معماری عصبی مغز آموزش داده می‌شوند". یکی از معماری‌های مورد استفاده در پردازش زبان طبیعی (NLP) یک شبکه عصبی مبتنی بر یک است که برای اولین بار در سال ۲۰۱۷ با نام ترنسفورمر(Transformer) معرفی شد. مدل‌های GPT-n بر اساس این معماری شبکه عصبی مبتنی بر یادگیری عمیق است. تعدادی سیستم NLP وجود دارد که قادر به پردازش، استخراج، سازماندهی، اتصال، یافتن تضاد، درک و تولید پاسخ به سوالات هستند.
در ۱۱ ژوئن سال ۲۰۱۸، محققان و مهندسان اوپن ای آی مقاله اصلی خود را پیرامون مدل‌های مولد - مدل‌های زبانی - سیستم‌های هوش مصنوعی منتشر کرد که در رابطه با این بود که می‌توان سیستم‌ها را از قبل با مجموعه عظیمی از داده‌های یک دیتابیس (یادگیری ماشین)، در یک فرایند که آن را به نام پیش مولد آموزش (GP) می‌شناسیم آموزش داد. نویسندگان توضیح دادند که چگونه عملکردهای درک زبان در پردازش زبان طبیعی (NLP) در GPT-n از طریق فرایند «آموزش پیش فرض یک مدل زبان بر روی مجموعه متن متن بدون برچسب، و به دنبال آن تنظیم دقیق متمایز کننده در هر مورد خاص» بهبود یافته‌است. این امر نیازی به نظارت انسانی و برچسب زنی دستی که زمان زیادی نیاز دارد ندارد.
در فوریه ۲۰۲۰، مایکروسافت نسل طبیعی زبان تورینگ (T-NLG) خود را معرفی کر، که ادعا می‌شود «بزرگترین مدل زبانی تا کنون است که با ۱۷ میلیارد پارامتر منتشر شده‌است». با وجود کوچکتر بودن از مدل زبانی آی‌بی‌ام تانگورا که بیش از ۸ تریلیون پارامتر داشت، در انواع مختلفی از کارها که شامل جمع‌بندی متون و پاسخ به سوالات بود، بهتر از هر مدل زبانی دیگر عمل کرد.
در ۲۸ مه ۲۰۲۰، پیش چاپ آرکایو توسط گروهی از ۳۱ مهندس و محقق در اوپن ای آی توسعه GPT-3، نسل سوم «مدل زبان پیشرفته» را مطرح کرد. این تیم ظرفیت GPT-3 را بیش از دو مرتبه بیشتر از مدل قبلی خود، GPT-2، افزایش داد و GPT-3 را به عنوان بزرگترین مدل زبانی غیر پراکنده تا به امروز تبدیل کرد. &#58;&#8202;14&#8202; از آنجایی که GPT-3 از نظر ساختاری شبیه به مدلهای قبلی خود است، سطح بالاتر دقت در آن به افزایش ظرفیت و تعداد بیشتر پارامترها نسبت داده می‌شود. ظرفیت GPT-3 ده برابر ظرفیت Turing NLGمایکروسافت، بزرگترین مدل NLP بعدی است.
شصت درصد از مجموعه داده‌های پیش آموزشی GPT-3 از نسخه فیلتر شده Common Crawl متشکل از ۴۱۰ میلیارد توکن رمزگذاری شده با جفت بایت می‌باشد &#58;&#8202;9&#8202;منابع دیگر عبارتند از ۱۹ میلیارد توکن از WebText2 که ۲۲٪ کل حجم را نشان می‌دهد، ۱۲ میلیارد توکن از Books1 که ۸٪ را نشان می‌دهد، ۵۵ میلیارد توکن از Books2 که ۸٪ را نشان می‌دهد و ۳ میلیارد توکن از ویکی‌پدیا که ۳٪ را نشان می‌دهد. &#58;&#8202;9&#8202;GPT-3 بر روی صدها میلیارد کلمه آموزش دیده‌است و قادر به کدگذاری در CSS ,JSX ،Python و سایر موارد است. از آنجا که داده‌های آموزشی GPT-3 همه‌جانبه بوده، نیازی به آموزش بیشتر برای کارهای زبانی متمایز ندارد. داده‌های آموزشی شامل (occasional toxic language) است و GPT-3 گاهی اوقات در نتیجه تقلید از داده‌های آموزشی خود، زبان (occasional toxic) تولید می‌کند. مطالعه ای از دانشگاه واشینگتن نشان داد که GPT-3 دارای زبانی (occasional toxic) در سطح سمیت قابل مقایسه با مدلهای مشابه پردازش زبان طبیعی GPT-2 و CTRL است. GPT-3 در مقایسه با مدل قبلی خود، GPT-1، زبان سمی کمتری تولید کرد، اگرچه در مقایسه با CTRL Wiki، یک مدل زبانی که به‌طور کامل بر روی داده‌های ویکی‌پدیا آموزش داده‌است، هم نسل‌های بیشتری تولید کرد و هم سمیت بیشتری از زبان‌های سمی ایجاد کرد.
پی‌نوشت: سمی بودن زبان به معنای استفاده از کلماتی است که توهین آمیز هستند و با استفاده کردن ماشین از این کلمات موجب ترک کاربر می‌شوند.
در ۱۱ ژوئن ۲۰۲۰، اوپن ای آی اعلام کرد که کاربران می‌توانند درخواست دسترسی به API کاربر پسند GPT-3، یک «مجموعه ابزاره یادگیری ماشین» برای کمک به اوپن ای آی برای «کشف نقاط قوت و محدودیت» این فناوری جدید را داشته باشند. در این دعوتنامه توضیح داده شده‌است که چگونه این واسط برنامه‌نویسی کاربردی دارای رابط کاربری عمومی "text in, text out" است که می‌تواند تقریباً هر کاری که زبان انگلیسی را به جای مورد استفاده معمول تکمیل کند. به گفته یکی از کاربران، که به نسخه خصوصی اولیه OpenAI GPT-3 API دسترسی داشت، GPT-3 در نوشتن «متن منسجم شگفت انگیز» تنها با چند دستور ساده «بسیار خوب» بود. در آزمایش اولیه از ۸۰ فرد آمریکایی خواسته شد قضاوت کنند که آیا مقاله‌های کوتاه ۲۰۰ پوندی توسط انسان یا GPT-3 نوشته شده‌است. شرکت کنندگان در ۴۸ درصد مواقع قضاوت نادرست کردند و فقط کمی بهتر از حدس زدن تصادفی عمل کردند.
از آنجا که GPT-3 می‌تواند «مقالاتی خبری تولید کند که ارزیابی آنها از مقالاتی که توسط انسان نوشته شده‌است مشکل است»، GPT-3 دارای «پتانسیل پیشبرد کاربردهای مفید و مضر مدلهای زبانی است.» &#58;&#8202;34&#8202;در مقاله خود در ۲۸ مه ۲۰۲۰، محققان به‌طور مفصل "اثرات مضر GPT-3" که شامل «اطلاعات غلط، هرزنامه، فیشینگ، سوء استفاده از فرایندهای قانونی و دولتی، مقاله نویسی متقلبانه و بهانه‌گیری و مهندسی اجتماعی» است. نویسندگان توجه خود را به این خطرات جلب کرده و خواستار تحقیق دربارهٔ کاهش ریسک می‌شوند. &#58;&#8202;34&#8202;
GPT-3 از طریق endpoint در پایتون و Curl، یا با دسترسی به یک playground رسمی مبتنی بر وب، یک رابط تبدیل متن خارج از متن را ارائه می‌دهد.
یک کاربر می‌تواند مقداری متن را به عنوان درخواست وارد کند و مدل یک تکمیل متن ایجاد می‌کند که سعی می‌کند با هر زمینه یا الگویی که داده شده مطابقت داشته باشد.
مدلها، متن را با تفکیک آن به توکن درک و پردازش می‌کنند. نشانه‌ها می‌توانند کلمات یا فقط تکه‌های کاراکتر باشند. به عنوان مثال، کلمه "همبرگر" به نشانه "ham" , "bur" و "ger" تقسیم می‌شود، در حالی که یک کلمه کوتاه و رایج مانند "گلابی" یک نشانه واحد است. بسیاری از نشانه‌ها با یک فضای سفید شروع می‌شوند، به عنوان مثال "سلام" و "خداحافظ".
مدل‌های پایه GPT-3 می‌توانند زبان طبیعی را درک کرده و تولید کنند. پایه شامل ۴ موتور، به نام davinci ، curie ، babbage و ada با سطوح مختلف از قدرت مناسب برای کارهای متفاوت است.
مدلهای Codex فرزندان مدلهای پایه GPT-3 هستند که می‌توانند کد را درک کرده و تولید کنند. داده‌های آموزشی آنها شامل زبان طبیعی و کد عمومی GitHub است.
اوپن ای آی ابتدا در سال ۲۰۱۵ به صورت غیرانتفاعی تأسیس شد. در سال ۲۰۱۹، اوپن ای آی مدل پیش ساز GPT-3 را به صورت عمومی منتشر نکرد، و از شیوه‌های منبع باز قبلی OpenAI شکایت کرد و نگران بود که این مدل باعث تداوم اخبار جعلی شود. OpenAI سرانجام نسخه GPT-2 را منتشر کرد که ۸ درصد از اندازه مدل اصلی بود. در همان سال، OpenAI تغییر شکل داد و یک شرکت انتفاعی بود. در سال ۲۰۲۰، مایکروسافت اعلام کرد که این شرکت دارای مجوز انحصاری GPT-3 برای محصولات و خدمات مایکروسافت پس از سرمایه‌گذاری چند میلیارد دلاری در اوپن ای آی است. این توافقنامه به اوپن ای آی اجازه می‌دهد تا یک API رو به عموم ارائه دهد تا کاربران بتوانند متن را به GPT-3 ارسال کنند تا خروجی مدل را دریافت کنند، اما فقط مایکروسافت به کد منبع GPT-3 دسترسی خواهد داشت.
مدلهای بزرگ زبانی، مانند GPT-3، به دلیل تأثیر محیطی آموزش و ذخیره مدلها، مورد انتقاد محققان اخلاق هوش مصنوعی گوگل قرار گرفته‌اند، که در مقاله ای که توسط تیمنیت گبرو و امیلی M. بندر در سال ۲۰۲۱ منتشر شد آماده است
